{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_QAE_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iwZJJ8SvsjSD",
        "KIw36PLBuFsJ",
        "CY37dG6Cr6pr",
        "r9RVluTUsXZu",
        "d7DUAhqys2ri",
        "hgtiSMG18OqG",
        "IBY4mxOxtQAp",
        "U5tSOQtethMi",
        "NWYmgR3bM-Wn"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwZJJ8SvsjSD",
        "colab_type": "text"
      },
      "source": [
        "# load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9fOm_xDtUGa",
        "colab_type": "code",
        "outputId": "d9ba9f7b-c1b6-4f81-9c89-cb3bcc2ac087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.15\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tensorflow.data import Iterator\n",
        "from tensorflow.contrib import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.2.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.28.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (46.1.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=a757345fa800f6cd587412a6dccf9c9cbca773402358b3f7b6332295372ecc9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement tensorflow-estimator<2.3.0,>=2.2.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "TensorFlow 1.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIw36PLBuFsJ",
        "colab_type": "text"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hB8mmpTuIhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_img(img):\n",
        "\t# processes grayscale png images\n",
        "\timg = tf.image.decode_png(img,channels=1)\n",
        "\timg = tf.image.convert_image_dtype(img,\n",
        "\t\ttf.uint8)\n",
        "\treturn img\n",
        "\n",
        "def process_path(file_path):\n",
        "\t# reads and decodes png file \n",
        "\timg = tf.io.read_file(file_path)\n",
        "\timg = decode_img(img)\n",
        "\treturn img\n",
        "\n",
        "def load_set(img_path, set_type='train'):\n",
        "\tlist_ds = tf.data.Dataset.list_files(\n",
        "\t\tstr(img_path/'*.png'))\n",
        "\tdataset = list_ds.map(process_path,\n",
        "\t\tnum_parallel_calls=\n",
        "\t\ttf.data.experimental.AUTOTUNE)\n",
        "\treturn dataset\n",
        "\n",
        "def load_data(dataset='mias',DIR = Path('.')):\n",
        "\t\n",
        "\tif dataset == 'mias':\n",
        "\t\tpath = DIR/'mias_dataset'\n",
        "\telif dataset == 'mias_augmented':\n",
        "\t\tpath = DIR/'mias_dataset_augmented'\n",
        "\telif dataset == 'dental':\n",
        "\t\tpath = DIR/'dental_dataset'\n",
        "\telif dataset == 'dental_augmented':\n",
        "\t\tpath = DIR/'dental_dataset_augmented'\n",
        "\telif dataset == 'both_augmented':\n",
        "\t\tpath = DIR/'both_datasets_augmented'\n",
        "\telse:\n",
        "\t\tpath = DIR/'both_datasets'\n",
        "\n",
        "\ttrain_path = path/'train'\n",
        "\tval_path = path/'val'\n",
        "\ttest_path = path/'test'\n",
        "\n",
        "\ttrain_ds = load_set(train_path,\n",
        "\t                    set_type='train')\n",
        "\tval_ds = load_set(val_path,\n",
        "\t                  set_type='val')\n",
        "\ttest_ds = load_set(test_path,\n",
        "\t                   set_type='test')\n",
        "\n",
        "\treturn train_ds,val_ds,test_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY37dG6Cr6pr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# loss functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgaPUruUsK6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def huber_loss(prediction, ground_truth,\n",
        "                delta=1.0):\n",
        "    # set a small delta value to make \n",
        "    # it behave like L1 loss\n",
        "    loss = tf.keras.losses.Huber(\n",
        "        delta=delta)\n",
        "    return loss(prediction, ground_truth)\n",
        "\n",
        "\n",
        "def l2_loss(prediction, ground_truth):\n",
        "\n",
        "    loss = tf.nn.l2_loss(\n",
        "        prediction - ground_truth)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def ssim_loss(prediction, ground_truth, \n",
        "              max_val, filter_size=11, \n",
        "              filter_sigma=1.5, k1=0.01, \n",
        "              k2=0.03):\n",
        "    # Gaussian filter of size 11x11 and \n",
        "    # width 1.5 is used. \n",
        "    # Image has to be at least 11x11 big.\n",
        "\n",
        "    ssim_loss = tf.image.ssim(\n",
        "        prediction,\n",
        "        ground_truth,\n",
        "        max_val,\n",
        "        filter_size=filter_size,\n",
        "        filter_sigma=filter_sigma,\n",
        "        k1=k1,\n",
        "        k2=k2)\n",
        "    return 1.0-tf.math.reduce_mean(ssim_loss)\n",
        "\n",
        "def compute_psnr(ref, target):\n",
        "  # correlated with MSE / L2\n",
        "    diff = target - ref\n",
        "    sqr = tf.multiply(diff, diff)\n",
        "    err = tf.reduce_sum(sqr)\n",
        "    v = tf.reduce_prod(diff.get_shape())\n",
        "    mse = err / tf.cast(v, tf.float32)+1e-12\n",
        "    psnr = 10. * (tf.log(\n",
        "        255.* 255./ mse)/tf.log(10.))\n",
        "\n",
        "    return psnr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9RVluTUsXZu",
        "colab_type": "text"
      },
      "source": [
        "# additive noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFPv3pOwsw-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian_noise(image,proportion=0.2,\n",
        "\tmean=0, std=1):\n",
        "    '''Apply gaussina noise of given mean,\n",
        "    \tstandard deviation and proportion\n",
        "    \tor scale of the noise\n",
        "    Args:\n",
        "        image: Tensor [batch, height, width]\n",
        "        proportion: scalar - scale of noise\n",
        "        mean: scalar\n",
        "        std: scalar - standard deviation\n",
        "    Returns:\n",
        "        Tensor of same dtype and shape as\n",
        "        image\n",
        "    '''\n",
        "\n",
        "    noise = proportion*tf.random.normal(\n",
        "        shape=image.get_shape().as_list(),\n",
        "        mean=mean,\n",
        "        stddev=std,\n",
        "        dtype=tf.float32)\n",
        "    image = tf.cast(image,\n",
        "        tf.float32) / 255.0 + noise\n",
        "\n",
        "    img_min = tf.math.reduce_min(image)\n",
        "    img_max = tf.math.reduce_max(image)\n",
        "\n",
        "    img_range = img_max - img_min\n",
        "\n",
        "    image = tf.cast(\n",
        "    \t255*(image - img_min)/img_range,\n",
        "    \ttf.uint8)\n",
        "\n",
        "    return image\n",
        "\n",
        "def poisson_noise(image,proportion=0.2,\n",
        "\tlam=1):\n",
        "    '''Apply poisson noise of given lambda,\n",
        "    \tand proportion/scale of the noise\n",
        "    \tImplemented as additive noise\n",
        "    Args:\n",
        "        image: Tensor [batch, height, width]\n",
        "        proportion: scalar - scale of noise\n",
        "        lam: scalar - Expected value of \n",
        "                      poisson\n",
        "    Returns:\n",
        "        Tensor of same dtype and shape as\n",
        "        image\n",
        "    '''\n",
        "\n",
        "    noise = proportion*tf.random.poisson(\n",
        "        shape=image.get_shape().as_list(),\n",
        "        lam=lam, dtype=tf.float32)\n",
        "    image = tf.cast(image,\n",
        "        tf.float32) / 255.0 + noise\n",
        "\n",
        "    img_min = tf.math.reduce_min(image)\n",
        "    img_max = tf.math.reduce_max(image)\n",
        "\n",
        "    img_range = img_max - img_min\n",
        "\n",
        "    image = tf.cast(\n",
        "    \t255*(image - img_min)/img_range,\n",
        "    \ttf.uint8)\n",
        "\n",
        "    return image\n",
        "\n",
        "def random_noise(image,proportion=0.2,\n",
        "\tstd_range=[1,5], lam_range=[1,5]):\n",
        "\t'''Randomly apply gaussian or poisson noise \n",
        "\t\twith stdev and lambda selected randomly \n",
        "\t\tfrom the range\n",
        "    Args:\n",
        "        image: Tensor [batch, height, width]\n",
        "        proportion: scalar - scale of noise\n",
        "        std_range: '2D list of int'\n",
        "        lam_range: '2D list of int'\n",
        "    Returns:\n",
        "        Tensor of same dtype and shape as\n",
        "        image\n",
        "    '''\n",
        "\n",
        "\tlam = tf.cast(\n",
        "\t\ttf.random.shuffle(tf.constant(\\\n",
        "\t\tlist(range(lam_range[0],lam_range[1])))),\n",
        "\t\ttf.float32)\n",
        "\n",
        "\tstd = tf.cast(\n",
        "\t\ttf.random.shuffle(tf.constant(\\\n",
        "\t\tlist(range(std_range[0],std_range[1])))),\n",
        "\t\ttf.float32)\n",
        "\n",
        "    # conditionally add noise\n",
        "\timage = tf.cond(tf.random.uniform(\n",
        "            shape=[],\n",
        "            dtype=tf.float32) > 0.5,\n",
        "        lambda: gaussian_noise(image,\\\n",
        "        \tproportion=proportion, std=std[0]),\n",
        "        lambda: poisson_noise(image,\\\n",
        "        \tproportion=proportion, lam=lam[0]))\n",
        "    \n",
        "\treturn image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7DUAhqys2ri",
        "colab_type": "text"
      },
      "source": [
        "# denoising autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgN_vO5tAMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_layer(input,layer_name,shape,padding='SAME',\n",
        "               batch_norm=False,max_pool=False,\n",
        "               activation_fn=None,is_training=False):\n",
        "   '''Creates a conv/ conv+maxpool/ conv+batchnorm\n",
        "      or conv+maxpool+batchnorm layer based on \n",
        "      arguments\n",
        "    Args:\n",
        "        input: Tensor [batch, height, width, in_channel]\n",
        "        shape: array/list [filter_height, filter_width,\n",
        "                            in_channel, out_channel]\n",
        "        padding: string 'SAME' or 'VALID'\n",
        "        activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "    Returns:\n",
        "        Tensor with out_channel channels and dimensions\n",
        "        might vary with padding type\n",
        "    '''\n",
        "    \n",
        "    with tf.variable_scope(layer_name):\n",
        "      w_init = tf.keras.initializers.TruncatedNormal(\n",
        "          stddev=0.1,dtype=tf.float32)\n",
        "      b_init = tf.constant_initializer(0.0)\n",
        "\n",
        "      W = tf.get_variable('w',shape=shape,\n",
        "                          initializer=w_init)\n",
        "      b = tf.get_variable('b',shape=[shape[3]],\n",
        "                          initializer=b_init)\n",
        "      \n",
        "      conv_out = tf.nn.conv2d(\n",
        "          input, W, strides=[1, 1, 1, 1],\n",
        "          padding=padding, name='conv_2d')\n",
        "      \n",
        "      conv_out = tf.nn.bias_add(conv_out,b)\n",
        "\n",
        "      if batch_norm:\n",
        "        conv_out = tf.layers.batch_normalization(\n",
        "            conv_out,scale=True,center=True,\n",
        "            training=is_training, name='batch_norm')\n",
        "      \n",
        "      if max_pool:\n",
        "        conv_out = tf.nn.max_pool2d(\n",
        "            conv_out,ksize=[1,2,2,1],\n",
        "            strides=[1,2,2,1],padding='VALID')\n",
        "        \n",
        "      if activation_fn != None:\n",
        "        conv_out = activation_fn(conv_out)\n",
        "      \n",
        "      return conv_out\n",
        "\n",
        "def deconv_layer(input,out_filters,ksize=(2,2),\n",
        "                 strides=(2,2),padding='SAME'):\n",
        "\n",
        "    deconv_out = tf.layers.conv2d_transpose(\n",
        "        input, filters=out_filters,\n",
        "        kernel_size=ksize, strides=strides,\n",
        "        padding=padding)\n",
        "    \n",
        "    return deconv_out\n",
        "\n",
        "def upsample_layer(input,ksize=(2,2),\n",
        "                   interp='nearest'):\n",
        "    upsample = tf.keras.layers.UpSampling2D(\n",
        "        size=ksize,interpolation=interp)\n",
        "    \n",
        "    return upsample(input)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AktvlDNSuNnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_DAE(x_input,k_size,num_filters,\n",
        "                    activation_fn,fin_activation_fn,\n",
        "                    is_training=False,\n",
        "                    verbose=False,batch_norm=False):\n",
        "   '''Build a denoising autoencoder network as \n",
        "      discussed by L. Gondara. 'Medical image \n",
        "      denoising using convolutional denoising \n",
        "      autoencoders.' In2016IEEE 16th International \n",
        "      Conference on Data Mining Workshops (ICDMW), \n",
        "      pages 241–246,2016\n",
        "    Args:\n",
        "        x_input: Tensor [batch, height, width, in_channel]\n",
        "        k_size: array/list [filter_height, filter_width]\n",
        "        num_filters: scalar - starting number of filters\n",
        "        activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "        fin_activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "    Returns:\n",
        "        prediction: Tensor of the same shape as x_input\n",
        "    '''\n",
        "  # encoder bit\n",
        "  encode_conv1 = conv_layer(\n",
        "      x_input,'econv1', \n",
        "      shape=[k_size, k_size, 1,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=True, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  encode_conv2 = conv_layer(\n",
        "      encode_conv1,'econv2',\n",
        "      shape=[k_size,k_size,num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=True, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  encode_conv3 = conv_layer(\n",
        "      encode_conv2,'econv3',\n",
        "      shape=[k_size,k_size,num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  \n",
        "  # decoder bit\n",
        "  decode_conv6 = upsample_layer(encode_conv3,\n",
        "                                ksize=(2,2),\n",
        "                                interp='nearest')\n",
        "\n",
        "  decode_conv5 = conv_layer(\n",
        "      decode_conv6,'dconv5',\n",
        "      shape=[k_size,k_size,num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "\n",
        "  decode_conv4 = upsample_layer(decode_conv5,\n",
        "                                ksize=(2,2),\n",
        "                                interp='nearest')\n",
        "\n",
        "  decode_conv3 = conv_layer(\n",
        "      decode_conv4,'dconv3',\n",
        "      shape=[k_size,k_size,num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "\n",
        "  # decode_conv2 = upsample_layer(decode_conv3,\n",
        "  #                               ksize=(2,2),\n",
        "  #                               interp='nearest')\n",
        "\n",
        "  prediction = conv_layer(\n",
        "      decode_conv4,'dconv1',\n",
        "      shape=[k_size, k_size, num_filters, 1],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=fin_activation_fn,\n",
        "      is_training=is_training)\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgtiSMG18OqG",
        "colab_type": "text"
      },
      "source": [
        "# linear_AE_skip_connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy5Mrcmm8N9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_AE_linear_skip(x_input,k_size,num_filters,\n",
        "                         activation_fn,fin_activation_fn,\n",
        "                         is_training=False,\n",
        "                         verbose=False,batch_norm=False):\n",
        "     '''Build a denoising autoencoder network similar \n",
        "      to Denoising Autoencoder proposed by L.Gondara.\n",
        "      With added skip connections from encoder to decoder\n",
        "    Args:\n",
        "        x_input: Tensor [batch, height, width, in_channel]\n",
        "        k_size: array/list [filter_height, filter_width]\n",
        "        num_filters: scalar - starting number of filters\n",
        "        activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "        fin_activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "    Returns:\n",
        "        prediction: Tensor of the same shape as x_input\n",
        "    '''\n",
        "  # encoder bit\n",
        "  encode_conv1 = conv_layer(\n",
        "      x_input,'econv1', \n",
        "      shape=[k_size, k_size, 1,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=True, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  encode_conv2 = conv_layer(\n",
        "      encode_conv1,'econv2',\n",
        "      shape=[k_size,k_size,num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=True, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  encode_conv3 = conv_layer(\n",
        "      encode_conv2,'econv3',\n",
        "      shape=[k_size,k_size,num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=True, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  encode_conv4 = conv_layer(\n",
        "      encode_conv3,'econv4',\n",
        "      shape=[k_size,k_size,num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  # decoder bit\n",
        "  decode_conv6 = upsample_layer(encode_conv4,\n",
        "                                ksize=(2,2),\n",
        "                                interp='nearest')\n",
        "\n",
        "  decode_conv5 = conv_layer(\n",
        "      tf.concat([decode_conv6,encode_conv2],axis=-1),\n",
        "      'dconv5',\n",
        "      shape=[k_size,k_size,2*num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "\n",
        "  decode_conv4 = upsample_layer(decode_conv5,\n",
        "                                ksize=(2,2),\n",
        "                                interp='nearest')\n",
        "  \n",
        "  decode_conv3 = conv_layer(\n",
        "      tf.concat([decode_conv4,encode_conv1],axis=-1),\n",
        "      'dconv3',\n",
        "      shape=[k_size,k_size,2*num_filters,num_filters],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "\n",
        "  decode_conv2 = upsample_layer(decode_conv3,\n",
        "                                ksize=(2,2),\n",
        "                                interp='nearest')\n",
        "\n",
        "  prediction = conv_layer(\n",
        "      decode_conv2,x_input, 'dconv1',\n",
        "      shape=[k_size,k_size,num_filters,1],\n",
        "      padding='SAME', batch_norm=batch_norm,\n",
        "      max_pool=False, activation_fn=fin_activation_fn,\n",
        "      is_training=is_training)\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBY4mxOxtQAp",
        "colab_type": "text"
      },
      "source": [
        "# original quadratic autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVcVlrLKtYLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   ''' Quadratic autoencoder code provided by\n",
        "       Fenglei Fan as part of their work on \n",
        "       'Quadratic autoencoder (q-ae) for low-dose \n",
        "       ct denoising' .IEEE Transactions on \n",
        "       Medical Imaging,pages 1–1, 2019\n",
        "    '''\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(\n",
        "        -0.02, shape=shape,dtype=tf.float32)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def weight_variable_Wr(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1,dtype=tf.float32)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def weight_variable_Wg(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1,dtype=tf.float32)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def weight_variable_Wb(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.01,dtype=tf.float32)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d_valid(x, W):\n",
        "    return tf.nn.conv2d(\n",
        "        x, W, strides=[1, 1, 1, 1], \n",
        "        padding='VALID')\n",
        "\n",
        "def conv2d_same(x, W):\n",
        "    return tf.nn.conv2d(\n",
        "        x, W, strides=[1, 1, 1, 1],\n",
        "        padding='SAME')\n",
        "\n",
        "def deconv2d_valid(x,W,outputshape):\n",
        "    return tf.nn.conv2d_transpose(\n",
        "        x,W,output_shape=outputshape,\n",
        "        strides= [1,1,1,1],\n",
        "        padding = 'VALID')\n",
        "\n",
        "def deconv2d_same(x,W,outputshape):\n",
        "    return tf.nn.conv2d_transpose(\n",
        "        x,W,output_shape=outputshape,\n",
        "        strides= [1,1,1,1],\n",
        "        padding = 'SAME')\n",
        "\n",
        "def Quad_deconv_layer_valid_linear(\n",
        "    input, shape, outputshape):\n",
        "    \n",
        "    W_r = weight_variable_Wr(shape)\n",
        "    #W_g = weight_variable_Wg(shape)\n",
        "    W_g = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    #W_b = weight_variable_Wb(shape)\n",
        "    W_b = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    \n",
        "    b_r = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[2]],dtype=tf.float32))\n",
        "    b_g = tf.Variable(tf.constant(\n",
        "        1, shape=[shape[2]],dtype=tf.float32))\n",
        "    c = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[2]],dtype=tf.float32))\n",
        "    \n",
        "    return (deconv2d_valid(input, W_r, outputshape)+b_r)*\\\n",
        "      (deconv2d_valid(input, W_g,outputshape)+b_g)+\\\n",
        "      deconv2d_valid(input*input, W_b,outputshape)+c\n",
        "\n",
        "def Quad_deconv_layer_same_linear(\n",
        "    input, shape, outputshape):\n",
        "    \n",
        "    W_r = weight_variable_Wr(shape)\n",
        "    #W_g = weight_variable_Wg(shape)\n",
        "    W_g = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    #W_b = weight_variable_Wb(shape)\n",
        "    W_b = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    \n",
        "    b_r = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[2]],dtype=tf.float32))\n",
        "    b_g = tf.Variable(tf.constant(\n",
        "        1, shape=[shape[2]],dtype=tf.float32))\n",
        "    c = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[2]],dtype=tf.float32))\n",
        "    \n",
        "    return (deconv2d_same(input, W_r, outputshape)+b_r)*\\\n",
        "      (deconv2d_same(input, W_g,outputshape)+b_g)+\\\n",
        "      deconv2d_same(input*input, W_b,outputshape)+c\n",
        "\n",
        "def Quad_deconv_layer_same(\n",
        "    input, shape, outputshape,activation_fn):\n",
        "    \n",
        "    W_r = weight_variable_Wr(shape)\n",
        "    #W_g = weight_variable_Wg(shape)\n",
        "    W_g = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    #W_b = weight_variable_Wb(shape)\n",
        "    W_b = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    \n",
        "    b_r = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[2]],dtype=tf.float32))\n",
        "    b_g = tf.Variable(tf.constant(\n",
        "        1, shape=[shape[2]],dtype=tf.float32))\n",
        "    c = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[2]],dtype=tf.float32))\n",
        "    \n",
        "    return activation_fn(\n",
        "        (deconv2d_same(input, W_r, outputshape)+b_r)*\\\n",
        "        (deconv2d_same(input, W_g,outputshape)+b_g)+\\\n",
        "        deconv2d_same(input*input, W_b,outputshape)+c)\n",
        "\n",
        "\n",
        "def Quad_conv_layer_valid(\n",
        "    input, shape,activation_fn):\n",
        "    \n",
        "    W_r = weight_variable_Wr(shape)\n",
        "    #W_g = weight_variable_Wg(shape)\n",
        "    W_g = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    #W_b = weight_variable_Wb(shape)\n",
        "    W_b = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))  \n",
        "    # W_b can also be initialized by \n",
        "    # Gaussian with samll variance\n",
        "\n",
        "    b_r = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[3]],dtype=tf.float32))\n",
        "    b_g = tf.Variable(tf.constant(\n",
        "        1, shape=[shape[3]],dtype=tf.float32))\n",
        "    c = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[3]],dtype=tf.float32))\n",
        "    \n",
        "    return activation_fn(\n",
        "        (conv2d_valid(input, W_r)+b_r)*\\\n",
        "        (conv2d_valid(input, W_g)+b_g)+\\\n",
        "        conv2d_valid(input*input, W_b)+c) \n",
        "\n",
        "def Quad_conv_layer_same(\n",
        "    input, shape,activation_fn):\n",
        "    \n",
        "    W_r = weight_variable_Wr(shape)\n",
        "    #W_g = weight_variable_Wg(shape)\n",
        "    W_g = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    #W_b = weight_variable_Wb(shape)\n",
        "    W_b = tf.Variable(tf.constant(\n",
        "        0, shape=shape,dtype=tf.float32))\n",
        "    \n",
        "    b_r = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[3]],dtype=tf.float32))\n",
        "    b_g = tf.Variable(tf.constant(\n",
        "        1, shape=[shape[3]],dtype=tf.float32))\n",
        "    c = tf.Variable(tf.constant(\n",
        "        0, shape=[shape[3]],dtype=tf.float32))\n",
        "    \n",
        "    return activation_fn(\n",
        "        (conv2d_same(input, W_r)+b_r)*\\\n",
        "        (conv2d_same(input, W_g)+b_g)+\\\n",
        "        conv2d_same(input*input, W_b)+c) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC5QaLaPtyPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_QAE_original(x_input,k_size,num_filters,\n",
        "            activation_fn,is_training=False,\n",
        "\t\t\t\t\t\tverbose=False):\n",
        "\n",
        "\tencode_conv1 = Quad_conv_layer_same(\n",
        "\t\t\tx_input,\n",
        "\t\t\tshape=[k_size, k_size,1,num_filters],\n",
        "\t\t\tactivation_fn=activation_fn)\n",
        " \n",
        "\tencode_conv2 = Quad_conv_layer_same(\n",
        "\t\t\tencode_conv1,\n",
        "\t\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\t\tactivation_fn=activation_fn)\n",
        " \n",
        "\tencode_conv3 = Quad_conv_layer_same(\n",
        "\t\t\tencode_conv2,\n",
        "\t\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\t\tactivation_fn=activation_fn)\n",
        " \n",
        "\tencode_conv4 = Quad_conv_layer_same(\n",
        "\t\t\tencode_conv3,\n",
        "\t\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\t\tactivation_fn=activation_fn)\n",
        " \n",
        "\tencode_conv5 = Quad_conv_layer_valid(\n",
        "\t\t\tencode_conv4,\n",
        "\t\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\t\tactivation_fn=activation_fn)\n",
        "\n",
        "\tlayer_dict = {'encode_1':encode_conv1,\n",
        "\t\t'encode_2':encode_conv2,\n",
        "\t\t'encode_3':encode_conv3,\n",
        "\t\t'encode_4':encode_conv4,\n",
        "\t\t'encode_5':encode_conv5}\n",
        "\n",
        "\treturn encode_conv5, layer_dict\n",
        "\n",
        "\n",
        "def decoder_QAE_original(d_input,layer_dict,k_size,\n",
        "            num_filters,activation_fn,\n",
        "\t\t\t\t\t\tis_training=False,verbose=False):\n",
        "\t\n",
        "\tdecode_conv4 = activation_fn(\n",
        "\t\tQuad_deconv_layer_valid_linear(\n",
        "\t\t\td_input,\n",
        "\t\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\t\toutputshape=tf.shape(\n",
        "\t\t\t\tlayer_dict['encode_4']))+\n",
        "\t\tlayer_dict['encode_4'])\n",
        " \n",
        "\tdecode_conv3 = Quad_deconv_layer_same(\n",
        "\t\tdecode_conv4,\n",
        "\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\toutputshape=tf.shape(\n",
        "\t\t\tlayer_dict['encode_3']),\n",
        "\t\t\tactivation_fn=activation_fn)\n",
        " \n",
        "\tdecode_conv2 = activation_fn(\n",
        "\t\tQuad_deconv_layer_same_linear(\n",
        "\t\t\tdecode_conv3,\n",
        "\t\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\t\toutputshape=tf.shape(\n",
        "\t\t\t\tlayer_dict['encode_2']))+\n",
        "\t\tlayer_dict['encode_2'])\n",
        " \n",
        "\tdecode_conv1 = Quad_deconv_layer_same(\n",
        "\t\tdecode_conv2,\n",
        "\t\tshape=[k_size,k_size,num_filters,num_filters],\n",
        "\t\toutputshape=tf.shape(\n",
        "\t\t\tlayer_dict['encode_1']),\n",
        "\t\t\tactivation_fn=activation_fn)\n",
        "\n",
        "\treturn decode_conv1\n",
        "\n",
        "\n",
        "def build_QAE_original(x_input,k_size,num_filters,\n",
        "              activation_fn,fin_activation_fn,\n",
        "\t\t\t\t\t\t\tbatch_norm=False,\n",
        "\t\t\t\t\t\t\tis_training=False,\n",
        "\t\t\t\t\t\t\tverbose=False):\n",
        "\t\n",
        "\tencoder_out, layer_dict = encoder_QAE_original(\n",
        "\t\t\tx_input=x_input,k_size=k_size,\n",
        "\t\t\tnum_filters=num_filters,\n",
        "\t\t\tactivation_fn=activation_fn,\n",
        "\t\t\tis_training=is_training, verbose=verbose)\n",
        "\n",
        "\tdecoder_out = decoder_QAE_original(\n",
        "\t\t\td_input=encoder_out,layer_dict=layer_dict,\n",
        "\t\t\tk_size=k_size,num_filters=num_filters,\n",
        "\t\t\tactivation_fn=activation_fn,\n",
        "\t\t\tis_training=is_training, verbose=verbose)\n",
        "\n",
        "\tprediction = fin_activation_fn(\n",
        "\t\t\tQuad_deconv_layer_same_linear(\n",
        "\t\t\t\t\tdecoder_out,\n",
        "\t\t\t\t\tshape=[k_size, k_size,1,num_filters],\n",
        "\t\t\t\t\toutputshape=tf.shape(x_input))+x_input)\n",
        "\n",
        "\treturn prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5tSOQtethMi",
        "colab_type": "text"
      },
      "source": [
        "# quadratic autoencoder (QAE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g_dtQbEyxc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Quad_conv_same(input,layer_name,shape,\n",
        "                   activation_fn,\n",
        "                   max_pool=False,\n",
        "                   batch_norm=False,\n",
        "                   is_training=False):\n",
        "    '''Creates a quadratic conv/ conv+maxpool/ \n",
        "      conv+batchnorm or conv+maxpool+batchnorm \n",
        "      layer based on arguments\n",
        "      quad_conv = (conv(input,w_r)+b_r)*\n",
        "                  (conv(input,w_g)+b_g)+\n",
        "                  (conv(input,w_b)+c)\n",
        "    Args:\n",
        "        input: Tensor [batch, height, width, in_channel]\n",
        "        shape: array/list [filter_height, filter_width,\n",
        "                            in_channel, out_channel]\n",
        "        padding: string 'SAME' or 'VALID'\n",
        "        activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "    Returns:\n",
        "        Tensor with out_channel channels and dimensions\n",
        "        might vary with padding type\n",
        "    '''\n",
        "  \n",
        "    with tf.variable_scope(layer_name):\n",
        "      w_init = tf.keras.initializers.TruncatedNormal(\n",
        "          stddev=0.1,dtype=tf.float32)\n",
        "      # w_init = tf.keras.initializers.glorot_normal\n",
        "\n",
        "      W_r = tf.get_variable(\n",
        "          'w_r',shape=shape,initializer=w_init)\n",
        "      b_r = tf.get_variable(\n",
        "          'b_r',shape=[shape[3]],\n",
        "          initializer=tf.constant_initializer(0.0))\n",
        "      \n",
        "      W_g = tf.get_variable(\n",
        "          'w_g',shape=shape,\n",
        "          initializer=tf.constant_initializer(0.0))\n",
        "      b_g = tf.get_variable(\n",
        "          'b_g',shape=[shape[3]],\n",
        "          initializer=tf.constant_initializer(1.0))\n",
        "      \n",
        "      W_b = tf.get_variable(\n",
        "          'w_b',shape=shape,\n",
        "          initializer=tf.constant_initializer(0.0))\n",
        "      c = tf.get_variable(\n",
        "          'c',shape=[shape[3]],\n",
        "          initializer=tf.constant_initializer(0.0))\n",
        "      \n",
        "      conv_1 = tf.nn.conv2d(\n",
        "          input, W_r, strides=[1, 1, 1, 1],\n",
        "          padding='SAME',name='conv_2d_1')\n",
        "      conv_1 = tf.nn.bias_add(conv_1,b_r)\n",
        "\n",
        "      conv_2 = tf.nn.conv2d(\n",
        "          input, W_g, strides=[1, 1, 1, 1],\n",
        "          padding='SAME',name='conv_2d_2')\n",
        "      conv_2 = tf.nn.bias_add(conv_2,b_g)\n",
        "\n",
        "      conv_3 = tf.nn.conv2d(\n",
        "          input, W_b, strides=[1, 1, 1, 1],\n",
        "          padding='SAME',name='conv_2d_3')\n",
        "      conv_3 = tf.nn.bias_add(conv_3,c)\n",
        "\n",
        "      quad_conv_out = conv_1 * conv_2 + conv_3\n",
        "\n",
        "      if max_pool:\n",
        "        quad_conv_out = tf.nn.max_pool2d(\n",
        "            quad_conv_out,ksize=[1,2,2,1],\n",
        "            strides=[1,2,2,1],padding='VALID')\n",
        "        \n",
        "      if batch_norm:\n",
        "        quad_conv_out = tf.layers.batch_normalization(\n",
        "            quad_conv_out, scale=True,\n",
        "            center=True, training=is_training)\n",
        "\n",
        "      return activation_fn(quad_conv_out) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poMjmrGwKySg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_QAE(x_input,k_size,\n",
        "                    num_filters,\n",
        "                    activation_fn,\n",
        "                    is_training=False,\n",
        "                    verbose=False,\n",
        "                    batch_norm=False):\n",
        "\t# encoder chunck\n",
        "\tencode_conv1 = Quad_conv_same(\n",
        "      x_input,'econv1',\n",
        "      shape=[k_size,k_size,1,num_filters],\n",
        "      activation_fn=activation_fn,max_pool=True,\n",
        "      batch_norm=batch_norm,is_training=is_training)\n",
        " \n",
        "\tencode_conv2 = Quad_conv_same(\n",
        "      encode_conv1,'econv2',\n",
        "      shape=[k_size,k_size,num_filters,2*num_filters],\n",
        "      activation_fn=activation_fn,\n",
        "      max_pool=True,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        " \n",
        "\tencode_conv3 = Quad_conv_same(\n",
        "      encode_conv2,'econv3',\n",
        "      shape=[k_size,k_size,2*num_filters,4*num_filters],\n",
        "      activation_fn=activation_fn,\n",
        "      max_pool=True,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        "\n",
        "\tlayer_dict = {'encode_1':encode_conv1,\n",
        "\t\t'encode_2':encode_conv2,\n",
        "\t\t'encode_3':encode_conv3}\n",
        "\n",
        "\treturn encode_conv3, layer_dict\n",
        "\n",
        "def decoder_QAE(d_input,layer_dict,\n",
        "                       k_size,num_filters,\n",
        "                       activation_fn,\n",
        "                       is_training=False,\n",
        "                       verbose=False,\n",
        "                       batch_norm=False):\n",
        "  \n",
        "  upsample_3 = upsample_layer(d_input,\n",
        "                              ksize=(2,2),\n",
        "                              interp='nearest')\n",
        "\n",
        "  decode_conv3 = Quad_conv_same(\n",
        "      upsample_3,'dconv3',\n",
        "      shape=[k_size,k_size,4*num_filters,2*num_filters],\n",
        "      activation_fn=activation_fn,\n",
        "      max_pool=False,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  upsample_2 = upsample_layer(decode_conv3,\n",
        "                              ksize=(2,2),\n",
        "                              interp='nearest')\n",
        "  \n",
        "  decode_conv2 = Quad_conv_same(\n",
        "      upsample_2,'dconv2',\n",
        "      shape=[k_size,k_size,2*num_filters,num_filters],\n",
        "      activation_fn=activation_fn,\n",
        "      max_pool=False,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  upsample_1 = upsample_layer(decode_conv2,\n",
        "                              ksize=(2,2),\n",
        "                              interp='nearest')\n",
        "\n",
        "  return upsample_1\n",
        "\n",
        "\n",
        "\n",
        "def build_QAE(x_input,k_size,\n",
        "              num_filters,activation_fn,\n",
        "              fin_activation_fn,\n",
        "              is_training=False,\n",
        "              verbose=False,\n",
        "              batch_norm=False):\n",
        "  \n",
        "  '''Build a channel doubling quadratic \n",
        "    autoencoder\n",
        "    Args:\n",
        "        x_input: Tensor [batch, height, width, in_channel]\n",
        "        k_size: array/list [filter_height, filter_width]\n",
        "        num_filters: scalar - starting number of filters\n",
        "        activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "        fin_activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "    Returns:\n",
        "        prediction: Tensor of the same shape as x_input\n",
        "    '''\n",
        "  encoder_out, layer_dict = encoder_QAE(\n",
        "      x_input=x_input,k_size=k_size,\n",
        "      num_filters=num_filters,\n",
        "      activation_fn=activation_fn,\n",
        "      is_training=is_training, verbose=verbose,\n",
        "      batch_norm=batch_norm)\n",
        "  \n",
        "  decoder_out = decoder_QAE(\n",
        "      d_input=encoder_out,layer_dict=layer_dict,\n",
        "      k_size=k_size,num_filters=num_filters,\n",
        "      activation_fn=activation_fn,\n",
        "      is_training=is_training,\n",
        "      verbose=verbose, batch_norm=batch_norm)\n",
        "  \n",
        "  prediction = Quad_conv_same(\n",
        "      decoder_out,'dconv1',\n",
        "      shape=[k_size,k_size,num_filters,1],\n",
        "      activation_fn=fin_activation_fn,\n",
        "      max_pool=False,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        "   \n",
        "  return prediction\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWYmgR3bM-Wn",
        "colab_type": "text"
      },
      "source": [
        "# linear autoencoder (QAE architecture)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rat0YddcNDXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_linear(x_input,k_size,\n",
        "                    num_filters,\n",
        "                    activation_fn,\n",
        "                    is_training=False,\n",
        "                    verbose=False,\n",
        "                    batch_norm=False):\n",
        "\t\n",
        "  encode_conv1 = conv_layer(\n",
        "      x_input,'econv1',\n",
        "      shape=[k_size,k_size,1,num_filters],\n",
        "      padding='SAME',batch_norm=batch_norm,\n",
        "      max_pool=True, activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  encode_conv2 = conv_layer(\n",
        "      encode_conv1,'econv2',\n",
        "      shape=[k_size,k_size,num_filters,2*num_filters],\n",
        "      padding='SAME',batch_norm=batch_norm,\n",
        "      max_pool=True,activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  encode_conv3 = conv_layer(\n",
        "      encode_conv2,'econv3',\n",
        "      shape=[k_size,k_size,2*num_filters,4*num_filters],\n",
        "      padding='SAME',batch_norm=batch_norm,\n",
        "      max_pool=True,activation_fn=activation_fn,\n",
        "      is_training=is_training)\n",
        "\n",
        "  layer_dict = {'encode_1':encode_conv1,\n",
        "               'encode_2':encode_conv2,\n",
        "               'encode_3':encode_conv3}\n",
        "\n",
        "  return encode_conv3, layer_dict\n",
        "\n",
        "def decoder_linear(d_input,layer_dict,\n",
        "                   k_size,num_filters,\n",
        "                   activation_fn,\n",
        "                   is_training=False,\n",
        "                   verbose=False,\n",
        "                   batch_norm=False):\n",
        "\n",
        "  upsample_3 = upsample_layer(d_input,\n",
        "                              ksize=(2,2),\n",
        "                              interp='nearest')\n",
        "\n",
        "  decode_conv3 = conv_layer(\n",
        "      upsample_3,'dconv3',\n",
        "      shape=[k_size,k_size,4*num_filters,2*num_filters],\n",
        "      activation_fn=activation_fn,\n",
        "      max_pool=False,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  upsample_2 = upsample_layer(decode_conv3,\n",
        "                              ksize=(2,2),\n",
        "                              interp='nearest')\n",
        "  \n",
        "  decode_conv2 = conv_layer(\n",
        "      upsample_2,'dconv2',\n",
        "      shape=[k_size,k_size,2*num_filters,num_filters],\n",
        "      activation_fn=activation_fn,\n",
        "      max_pool=False,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        "  \n",
        "  upsample_1 = upsample_layer(decode_conv2,\n",
        "                              ksize=(2,2),\n",
        "                              interp='nearest')\n",
        "\n",
        "  return upsample_1\n",
        "\n",
        "\n",
        "\n",
        "def build_LAE(x_input,k_size,\n",
        "                     num_filters,\n",
        "                     activation_fn,\n",
        "                     fin_activation_fn,\n",
        "                     is_training=False,\n",
        "                     verbose=False,\n",
        "                     batch_norm=False):\n",
        "  '''Build a channel doubling linear neuron autoencoder\n",
        "    with the same architecture as QAE\n",
        "    Args:\n",
        "        x_input: Tensor [batch, height, width, in_channel]\n",
        "        k_size: array/list [filter_height, filter_width]\n",
        "        num_filters: scalar - starting number of filters\n",
        "        activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "        fin_activation_fn: function -None, tf.nn.relu,\n",
        "                                tf.nn.tanh\n",
        "    Returns:\n",
        "        prediction: Tensor of the same shape as x_input\n",
        "    '''\n",
        "  encoder_out, layer_dict = encoder_linear(\n",
        "      x_input=x_input,k_size=k_size,\n",
        "      num_filters=num_filters,\n",
        "      activation_fn=activation_fn,\n",
        "      is_training=is_training,\n",
        "      verbose=verbose, batch_norm=batch_norm)\n",
        "  \n",
        "  decoder_out = decoder_linear(\n",
        "      d_input=encoder_out,\n",
        "      layer_dict=layer_dict,\n",
        "      k_size=k_size, num_filters=num_filters,\n",
        "      activation_fn=activation_fn,\n",
        "      is_training=is_training,\n",
        "      verbose=verbose, batch_norm=batch_norm)\n",
        "  \n",
        "  prediction = conv_layer(\n",
        "      decoder_out,'dconv1',\n",
        "      shape=[k_size,k_size,num_filters,1],\n",
        "      activation_fn=fin_activation_fn,\n",
        "      max_pool=False,batch_norm=batch_norm,\n",
        "      is_training=is_training)\n",
        "   \n",
        "  return prediction\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xa6eADBu08u",
        "colab_type": "text"
      },
      "source": [
        "# Training Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7bDLk04yzJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttrDict(dict):\n",
        "\t# Helper class for argument management\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(\n",
        "\t\t\t\t\t\t*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def train(args):\n",
        "\n",
        "\t# Determine dataset size\n",
        "\tif 'augmented' in args.dataset:\n",
        "\t\ttrain_size = 750\n",
        "\telse:\n",
        "\t\ttrain_size = 300\n",
        "\n",
        "\t# Per epoch and total iterations\n",
        "\tepoch_iter = train_size//args.batch_size\n",
        "\titerations = args.epochs*epoch_iter\n",
        "\n",
        "\t# Path to this .ipynb document as well as data\n",
        "\t# current_dir = Path('.')/'drive'/'My Drive'/'JEB1433_Project'\n",
        "\tcurrent_dir = Path('.')\n",
        "\n",
        "\t# Checkpoint save paths\n",
        "\tif args.save_model and args.exp_name is 'grid_search':\n",
        "\t\texpt_folder = current_dir/args.exp_name\n",
        "\t\texpt_folder.mkdir(parents=True,exist_ok=True)\n",
        "\t\t\n",
        "\t\tsub_folder= '_lr_{0:.5f}_bs_{1:2d}'.format(\n",
        "\t\t\t\targs.learn_rate,args.batch_size)\n",
        "\t\tckpt_folder = expt_folder/sub_folder\n",
        "\t\tckpt_folder.mkdir(parents=True,exist_ok=True)\n",
        "\t\n",
        "\telif args.save_model and args.exp_name is not None:\n",
        "\t\tckpt_folder = current_dir/args.exp_name\n",
        "\t\tckpt_folder.mkdir(parents=True,exist_ok=True)\n",
        "\t\n",
        "\telse:\n",
        "\t\tckpt_folder = current_dir\n",
        "\n",
        "\tckpt_path = str(ckpt_folder.absolute())+'/'\n",
        "\n",
        "\t#load train and test datasets\n",
        "\tdata_dir = current_dir/'data'\n",
        "\ttrain_true,val_true,test_true=load_data(\n",
        "\t\t\tdataset=args.dataset,DIR=data_dir)\n",
        "\t\n",
        "\tif args.verbose:\n",
        "\t\tprint(args.dataset+' datasets loaded...')\n",
        "\n",
        "\t# repeat, batch and prefetch all datasets\n",
        "\ttrain_true = train_true.repeat().batch(\n",
        "\t\t\targs.batch_size)\n",
        "\ttrain_true = train_true.prefetch(\n",
        "\t\t\tbuffer_size=tf.data.experimental.AUTOTUNE)\n",
        " \n",
        "\tval_true = val_true.repeat().batch(\n",
        "\t\t\targs.batch_size)\n",
        "\tval_true = val_true.prefetch(\n",
        "\t\t\tbuffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\t\n",
        "\ttest_true = test_true.batch(\n",
        "\t\t\targs.batch_size)\n",
        "\ttest_true = test_true.prefetch(\n",
        "\t\t\tbuffer_size =tf.data.experimental.AUTOTUNE)\n",
        "\t\n",
        "\tif args.verbose:\n",
        "\t\tprint('Batches of {} created...'.format(args.batch_size))\n",
        " \n",
        "\t#create iterators for all datasets\n",
        "\titer_train_handle = train_true.make_one_shot_iterator().string_handle()\n",
        "\titer_val_handle = val_true.make_one_shot_iterator().string_handle()\n",
        "\titer_test_handle = test_true.make_one_shot_iterator().string_handle()\n",
        "\n",
        "\thandle = tf.placeholder(tf.string, shape=[])\n",
        "\n",
        "\titerator = Iterator.from_string_handle(handle,\n",
        "\t\ttf.compat.v1.data.get_output_types(train_true),\n",
        "\t\ttf.compat.v1.data.get_output_shapes(train_true))\n",
        "\n",
        "\tnext_batch = iterator.get_next()\n",
        "\n",
        "\n",
        "\t#load selected noise type for image preprocessing\n",
        "\tnoise_args = {'proportion':args.noise_proportion}\n",
        "\tif args.noise_type == 'random':\n",
        "\t\tnoise_fn = random_noise\n",
        "\t\tnoise_args['std_range'] = args.noise_std_range\n",
        "\t\tnoise_args['lam_range'] = args.noise_lam_range\n",
        "\telif args.noise_type == 'poisson':\n",
        "\t\tnoise_fn = poisson_noise\n",
        "\t\tnoise_args['lam'] = args.noise_lam\n",
        "\telse:\n",
        "\t\tnoise_fn = gaussian_noise\n",
        "\t\tnoise_args['mean'] = args.noise_mean\n",
        "\t\tnoise_args['std'] = args.noise_std\n",
        "\t\n",
        "\tif args.verbose:\n",
        "\t\tprint('Noise type: '+args.noise_type+\\\n",
        "\t\t      '. Parameters',noise_args)\n",
        "\n",
        "\t#add noise to images and cast to float for training\n",
        "\ttrue_img = tf.placeholder(tf.uint8, \n",
        "\t\tshape=[args.batch_size, 64, 64, 1])\n",
        "\tnoised_img = noise_fn(**noise_args,\n",
        "\t\timage=true_img)\n",
        "\tmodel_input = tf.cast(noised_img,\n",
        "\t\tdtype=tf.float32)\n",
        "\t\n",
        "\t# model output scaling factor based on final activation\n",
        "\toutput_scale_factor = 1.0\n",
        "\toutput_shift = 0.0\n",
        "\tif args.final_activation=='tanh':\n",
        "\t\tfin_activation_fn = tf.keras.activations.tanh\n",
        "\t\toutput_scale_factor = 127.5\n",
        "\t\toutput_shift = 1.0\n",
        "\telif args.final_activation=='sigmoid':\n",
        "\t\tfin_activation_fn = tf.keras.activations.sigmoid\n",
        "\t\toutput_scale_factor = 255.0\n",
        "\telse:\n",
        "\t\tfin_activation_fn = tf.nn.relu\n",
        "\n",
        "\tif args.activation=='tanh':\n",
        "\t\tactivation_fn = tf.keras.activations.tanh\n",
        "\telif args.activation=='sigmoid':\n",
        "\t\tactivation_fn = tf.keras.activations.sigmoid\n",
        "\telse:\n",
        "\t\tactivation_fn = tf.nn.relu\n",
        "\t\n",
        "\tif args.verbose:\n",
        "\t\tprint('Model activation function:',activation_fn.__name__)\n",
        "\t\tprint('Model output activation function:',fin_activation_fn.__name__)\n",
        " \n",
        "\t#SELECT MODEL TO LOAD\n",
        "\twith tf.device('/device:GPU:0'):\n",
        "\t\tif args.model=='DAE':\n",
        "\t\t\tmodel = build_DAE\n",
        "\t\telif args.model=='LAE':\n",
        "\t\t\tmodel = build_LAE\n",
        "\t\telif args.model=='QAE_original':\n",
        "\t\t\tmodel = build_QAE_original\n",
        "\t\t\targs.batch_norm = False\n",
        "\t\telif args.model=='QAE':\n",
        "\t\t\tmodel = build_QAE\n",
        "\t\telse:\n",
        "\t\t\tprint('selected model does not exist')\n",
        "\t\t\treturn\t\t\n",
        "\n",
        "\t\twith tf.variable_scope(args.model):\n",
        "\t\t\tdenoised_train_img = model(\n",
        "\t\t\t\t\tmodel_input,k_size=args.kernel_size,\n",
        "\t\t\t\t\tactivation_fn=activation_fn,\n",
        "\t\t\t\t\tfin_activation_fn=fin_activation_fn,\n",
        "\t\t\t\t\tnum_filters=args.num_filters,\n",
        "\t\t\t\t\tbatch_norm=args.batch_norm,\n",
        "\t\t\t\t\tis_training=True)\n",
        "\t\twith tf.variable_scope(args.model,reuse=tf.AUTO_REUSE):\n",
        "\t\t\tdenoised_val_img = model(\n",
        "\t\t\t\t\tmodel_input,k_size=args.kernel_size,\n",
        "\t\t\t\t\tactivation_fn=activation_fn,\n",
        "\t\t\t\t\tfin_activation_fn=fin_activation_fn,\n",
        "\t\t\t\t\tnum_filters=args.num_filters,\n",
        "\t\t\t\t\tbatch_norm=args.batch_norm,\n",
        "\t\t\t\t\tis_training=False)\n",
        "\n",
        "\tif args.verbose:\n",
        "\t\tprint('Model loaded: '+args.model)\n",
        "\t\n",
        "\n",
        "\t#load selected loss functions, selection of l2 and ssim\t\t\n",
        "\ttrain_l2_loss, train_ssim_loss = 0.0, 0.0\n",
        "\tval_l2_loss, val_ssim_loss = 0.0, 0.0\n",
        "\tl2_loss_weight, ssim_loss_weight = 0.0, 0.0\n",
        "\n",
        "\tscaled_denoised_train_image = output_scale_factor*(\n",
        "\t\t\t\t\toutput_shift+denoised_train_img)\n",
        "\n",
        "\tscaled_denoised_val_image = output_scale_factor*(\n",
        "\t\t\t\t\toutput_shift+denoised_val_img)\n",
        "\n",
        "\tif 'l2' in args.loss_type:\n",
        "\t\tl2_loss_weight = 1.0\n",
        "\t\ttrain_l2_loss = l2_loss(\n",
        "\t\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\t\tscaled_denoised_train_image)\n",
        "\t\tval_l2_loss = l2_loss(\n",
        "\t\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\t\tscaled_denoised_val_image)\t\n",
        "\tif 'ssim' in args.loss_type:\n",
        "\t\tssim_loss_weight = 1.0\n",
        "\t\ttrain_ssim_loss = ssim_loss(\n",
        "\t\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\t\tscaled_denoised_train_image,max_val=255)\n",
        "\t\tval_ssim_loss = ssim_loss(\n",
        "\t\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\t\tscaled_denoised_val_image,max_val=255)\n",
        "\t\t\n",
        "\t\n",
        "\tl2_loss_weight = l2_loss_weight/(\n",
        "\t\t\tlen(args.loss_type)*args.batch_size*2048*255*255)\n",
        "\tssim_loss_weight = ssim_loss_weight/\\\n",
        "\t\tlen(args.loss_type)\n",
        "\n",
        "\ttotal_train_loss = l2_loss_weight*train_l2_loss+\\\n",
        "\t\tssim_loss_weight*train_ssim_loss\n",
        "\n",
        "\ttotal_val_loss = l2_loss_weight*val_l2_loss+\\\n",
        "\t\tssim_loss_weight*val_ssim_loss\n",
        "\n",
        "\tif args.verbose:\n",
        "\t\tprint('Training losses: ',args.loss_type)\n",
        "\t\n",
        "\t# Evaluation Metrics\n",
        "\n",
        "\tnoisy_l2_loss = l2_loss(\n",
        "\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\ttf.cast(noised_img,dtype=tf.float32))\n",
        "\tnoisy_ssim_loss = ssim_loss(\n",
        "\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\ttf.cast(noised_img,dtype=tf.float32),\n",
        "\t\t\tmax_val=255)\n",
        "\tnoisy_metric_psnr = compute_psnr(\n",
        "\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\ttf.cast(noised_img,dtype=tf.float32))\n",
        "\n",
        "\ttest_metric_l2 = l2_loss(\n",
        "\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\tscaled_denoised_val_image)\n",
        "\ttest_metric_ssim = 1.0 - ssim_loss(\n",
        "\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\tscaled_denoised_val_image,max_val=255)\n",
        "\ttest_metric_psnr = compute_psnr(\n",
        "\t\t\ttf.cast(true_img,dtype=tf.float32),\n",
        "\t\t\tscaled_denoised_val_image)\n",
        " \n",
        "\ttotal_noisy_loss = l2_loss_weight*noisy_l2_loss+\\\n",
        "\t\tssim_loss_weight*noisy_ssim_loss\n",
        "\n",
        "\t#load optimizer for loss minimization\n",
        "\tif args.optimizer=='adam':\n",
        "\t\toptimizer = tf.train.AdamOptimizer(\n",
        "\t\t\t\tlearning_rate=args.learn_rate).minimize(\n",
        "\t\t\t\ttotal_train_loss)\n",
        "\telif args.optimizer=='gd':\n",
        "\t\toptimizer = tf.train.GradientDescentOptimizer(\n",
        "\t\t\t\tlearning_rate=args.learn_rate).minimize(\n",
        "\t\t\t\ttotal_train_loss)\n",
        "\telif args.optimizer=='momentum':\n",
        "\t\toptimizer = tf.train.MomentumOptimizer(\n",
        "\t\t\t\tlearning_rate=args.learn_rate,\n",
        "\t\t\t\tmomentum=0.9).minimize(total_train_loss)\n",
        "\telse:\n",
        "\t\tprint('Selected optimizer not available')\n",
        "\t\treturn\n",
        "\t\n",
        "\tif args.verbose:\n",
        "\t\tprint('Training optimzer:',args.optimizer)\t\n",
        "\n",
        "\t# tf summaries to keep track of losses\n",
        "\ttf.summary.scalar('train_l2_loss',train_l2_loss)\n",
        "\ttf.summary.scalar('train_ssim_loss',train_ssim_loss)\n",
        "\ttf.summary.scalar('total_train_loss',total_train_loss)\n",
        "\ttf.summary.scalar('val_l2_loss',val_l2_loss)\n",
        "\ttf.summary.scalar('val_ssim_loss',val_ssim_loss)\n",
        "\ttf.summary.scalar('total_val_loss',total_val_loss)\n",
        "\tmerged_summary = tf.summary.merge_all()\n",
        "\t\n",
        "\n",
        "\twith tf.Session().as_default() as sess:\n",
        "\t\tif args.exp_name is not None:\n",
        "\t\t\ttrain_writer = tf.summary.FileWriter(\n",
        "\t\t\t\t\tcurrent_dir/args.exp_name)\n",
        "\t\telse:\n",
        "\t\t\ttrain_writer = tf.summary.FileWriter(\n",
        "\t\t\t\t\tcurrent_dir/'train_data')\n",
        "\n",
        "\t\t# initialize all global and local variables\n",
        "\t\tinit_vars = tf.group(\n",
        "\t\t\t\ttf.global_variables_initializer(),\n",
        "\t\t\t\ttf.local_variables_initializer())\n",
        "\n",
        "\t\tsaver = tf.train.Saver()\n",
        "\t\t\n",
        "\t\tif args.verbose:\n",
        "\t\t\ttrain_params_count = np.sum(\n",
        "\t\t\t\t\t[np.prod(v.get_shape().as_list())\n",
        "\t\t \t\t\tfor v in tf.trainable_variables()])\n",
        "\t\t\tprint('Trainable parameters: {}'.\\\n",
        "\t\t\t      format(train_params_count))\n",
        "\t\t\t# for v in tf.trainable_variables():\n",
        "\t\t\t# \tprint(v.name+'. shape:{}'.format(v.shape))\n",
        "\t\t\t# return\n",
        "\t\t\tprint('')\n",
        "\t \n",
        "\t\tglobal_step = tf.train.get_global_step()\n",
        "\n",
        "\t\thandle_train,handle_val,handle_test = sess.run(\n",
        "\t\t\t[iter_train_handle, iter_val_handle,\n",
        "\t\t\t iter_test_handle])\n",
        "\n",
        "\t\tsess.run(init_vars)\n",
        "\n",
        "\t\ttrain_loss, val_loss = [], []\n",
        "\n",
        "\t\tfor step in range(iterations+1):\n",
        "\t\t\ttrain_true_img = sess.run(next_batch,\n",
        "\t\t\t\tfeed_dict={handle: handle_train})\n",
        "\n",
        "\t\t\tval_true_img = sess.run(next_batch,\n",
        "\t\t\t\tfeed_dict={handle: handle_val})\n",
        "\n",
        "\t\t\t_,t_loss,train_noise_loss = sess.run(\n",
        "\t\t\t\t\t[optimizer,total_train_loss,total_noisy_loss],\n",
        "\t\t\t\t\tfeed_dict={true_img:train_true_img})\n",
        "\t\t\t\n",
        "\t\t\tv_loss,val_noise_loss = sess.run(\n",
        "\t\t\t\t\t[total_val_loss,total_noisy_loss],\n",
        "\t\t\t\t\tfeed_dict = {true_img:val_true_img})\n",
        "\t\t\t\n",
        "\t\t\tif step%epoch_iter == 0:\n",
        "\t\t\t\ttrain_loss.append(t_loss)\n",
        "\t\t\t\tval_loss.append(v_loss)\n",
        "\t\t\t# train_writer.add_summary(t_summ,step)\n",
        "\n",
        "\t\t\tif args.verbose and step%epoch_iter == 0:\n",
        "\t\t\t\tprint('Epoch:{0:3d}/{1}, '\\\n",
        "\t\t\t\t      'Training Loss {2:.5f},...'\\\n",
        "\t\t\t\t\t\t\t'Noise Loss {3:.5f}'.format(\n",
        "\t\t\t\t\t\t\t\t\tstep//epoch_iter,args.epochs,\n",
        "\t\t\t\t\t\t\t\t\tt_loss,train_noise_loss))\n",
        "\t\t\t\tprint('Epoch:{0:3d}/{1}, '\\\n",
        "\t\t\t\t      'Validation Loss {2:.5f},...'\\\n",
        "\t\t\t\t\t\t\t'Noise Loss {3:.5f}'.format(\n",
        "\t\t\t\t\t\t\t\t\tstep//epoch_iter,args.epochs,\n",
        "\t\t\t\t\t\t\t\t\tv_loss,val_noise_loss))\n",
        "\t\t\n",
        "\t\t\tif args.visualize and step%(args.viz_every_epoch*epoch_iter)==0:\n",
        "\t\t\t\tinput_image = sess.run(\n",
        "\t\t\t\t\t\tmodel_input,\n",
        "\t\t\t\t\t\tfeed_dict={true_img:val_true_img})\n",
        "\t\t\t\toutput_image = sess.run(\n",
        "\t\t\t\t\t\tdenoised_val_img,\n",
        "\t\t\t\t\t\tfeed_dict={true_img:val_true_img})\n",
        "\t\t\t\tfig, axes = plt.subplots(\n",
        "\t\t\t\t\t\tnrows=1,ncols=3,figsize=(10,10))\n",
        "\t\t\t\taxes[0].axis('off')\n",
        "\t\t\t\taxes[1].axis('off')\n",
        "\t\t\t\taxes[2].axis('off')\n",
        "\t\t\t\taxes[0].imshow(val_true_img[0,:,:,0], cmap='gray')\n",
        "\t\t\t\taxes[1].imshow(input_image[0,:,:,0], cmap='gray')\n",
        "\t\t\t\taxes[2].imshow(output_image[0,:,:,0], cmap='gray')\n",
        "\t\t\t\tplt.draw()\n",
        "\t\tif args.plot:\n",
        "\t\t\tfig = plt.figure(frameon=True,figsize=(8,6))\n",
        "\t\t\tplt.plot(train_loss,label='Train loss')\n",
        "\t\t\tplt.plot(val_loss,label='Validation loss')\n",
        "\t\t\tplt.gca().tick_params(axis='both', which='major', labelsize=14)\n",
        "\t\t\tplt.xlabel('Epochs',fontsize=16)\n",
        "\t\t\tplt.ylabel('Total Loss',fontsize=16)\n",
        "\t\t\tplt.legend(loc=1,fontsize=14)\n",
        "\t\t\tplt.savefig(args.model+'_'+str(args.epochs)+\\\n",
        "\t\t\t            '_train_plot.png',format='png',\n",
        "\t\t\t            bbox_inches='tight')#,pad_inches = 0)\n",
        "\t\t\n",
        "\t\tif args.save_model:\n",
        "\t\t\tsaver.save(sess,ckpt_path)\n",
        "\n",
        "\t\tif args.run_inference:\t\n",
        "\t\t\ttest_summary = {}\n",
        "\n",
        "\t\t\ttest_l2, test_ssim, test_psnr = [], [], []\n",
        "\t\t\tnoisy_l2, noisy_ssim, noisy_psnr = [], [], []\n",
        "\n",
        "\t\t\tif args.verbose:\n",
        "\t\t\t\tprint('Starting test procedure')\n",
        "\t\t\tfor t_step in range(15):\n",
        "\n",
        "\t\t\t\ttest_true_img = sess.run(\n",
        "\t\t\t\t\t\tnext_batch,feed_dict={handle: handle_test})\n",
        "\t\t\t\t\n",
        "\t\t\t\tt_l2,t_ssim,t_psnr,in_l2,in_dssim,in_psnr = sess.run(\n",
        "\t\t\t\t\t\t[test_metric_l2,test_metric_ssim,\n",
        "\t\t\t\t\t\ttest_metric_psnr,noisy_l2_loss,\n",
        "\t\t\t\t\t\tnoisy_ssim_loss,noisy_metric_psnr],\n",
        "\t\t\t\t\t\tfeed_dict = {true_img:test_true_img})\n",
        "\n",
        "\t\t\t\tif args.visualize and t_step==10:\n",
        "\t\t\t\t\tinput_image = sess.run(\n",
        "\t\t\t\t\t\t\tmodel_input,\n",
        "\t\t\t\t\t\t\tfeed_dict={true_img:test_true_img})\n",
        "\t\t\t\t\toutput_image = sess.run(\n",
        "\t\t\t\t\t\t\tdenoised_val_img,\n",
        "\t\t\t\t\t\t\tfeed_dict={true_img:test_true_img})\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tfig, axes = plt.subplots(\n",
        "\t\t\t\t\t\t\tnrows=1,ncols=3,figsize=(10,10))\n",
        "\t\t\t\t\taxes[0].axis('off')\n",
        "\t\t\t\t\taxes[1].axis('off')\n",
        "\t\t\t\t\taxes[2].axis('off')\n",
        "\t\t\t\t\taxes[0].imshow(test_true_img[0,:,:,0], cmap='gray')\n",
        "\t\t\t\t\taxes[1].imshow(input_image[0,:,:,0], cmap='gray')\n",
        "\t\t\t\t\taxes[2].imshow(output_image[0,:,:,0], cmap='gray')\n",
        "\t\t\t\t\tplt.draw()\n",
        "\t\t\t\t\tplt.savefig(args.model+'_'+str(args.epochs)+\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t'_test_images.png',format='png',\n",
        "\t\t\t\t\t\t\t\t\t\t\tbbox_inches='tight',pad_inches = 0)\n",
        "\t\t\t\t\n",
        "\t\t\t\ttest_l2.append(t_l2)\n",
        "\t\t\t\ttest_ssim.append(t_ssim)\n",
        "\t\t\t\ttest_psnr.append(t_psnr)\n",
        "\n",
        "\t\t\t\tnoisy_l2.append(in_l2)\n",
        "\t\t\t\tnoisy_ssim.append(1.0-in_dssim)\n",
        "\t\t\t\tnoisy_psnr.append(in_psnr)\n",
        "\n",
        "\t\t\ttest_summary['test_l2']=np.mean(test_l2)\n",
        "\t\t\ttest_summary['test_ssim']=np.mean(test_ssim)\n",
        "\t\t\ttest_summary['test_psnr']=np.mean(test_psnr)\n",
        "\t\t\ttest_summary['noisy_l2']=np.mean(noisy_l2)\n",
        "\t\t\ttest_summary['noisy_ssim']=np.mean(noisy_ssim)\n",
        "\t\t\ttest_summary['noisy_psnr']=np.mean(noisy_psnr)\n",
        "\n",
        "\t\t\treturn test_summary\n",
        "\t\telse:\n",
        "\t\t\treturn train_loss,val_loss\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBI9KTCXvDVH",
        "colab_type": "text"
      },
      "source": [
        "# run training script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emUxFHwez3vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = AttrDict()\n",
        "model_list = ['QAE','QAE_original',\\\n",
        "              'LAE','DAE']\n",
        "args_dict = { \n",
        "              'model':model_list[2],\n",
        "              'kernel_size':3, \n",
        "              'num_filters':16, \n",
        "              'learn_rate':6e-4,\n",
        "              'optimizer': 'adam',\n",
        "              'activation':'tanh',\n",
        "              'final_activation':'tanh',\n",
        "              'batch_norm':False, \n",
        "              'batch_size':10, \n",
        "              'epochs':100,\n",
        "              'plot':True,\n",
        "              'visualize': True,\n",
        "              'viz_every_epoch':10, \n",
        "              'noise_type':'gaussian',\n",
        "              'noise_proportion':0.2,\n",
        "              'noise_mean':0,\n",
        "              'noise_std':1,\n",
        "              'noise_lam':1,\n",
        "              'noise_std_range':[1,5],\n",
        "              'noise_lam_range':[1,5],\n",
        "              'loss_type':['l2'],\n",
        "              'dataset':'dental_augmented',\n",
        "              'verbose':True,\n",
        "              'exp_name': 'test',\n",
        "              'save_model': False,\n",
        "              'run_inference': True,\n",
        "             \n",
        "}\n",
        "args.update(args_dict)\n",
        "# tf.reset_default_graph()\n",
        "# test_results = train(args)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABvI6Mg_RUvF",
        "colab_type": "code",
        "outputId": "eec6ca71-6130-4b05-b43a-74453a4fb075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def test_losses():\n",
        "  models = ['QAE','LAE']\n",
        "  losses = ['l2','ssim']\n",
        "  args.run_inference = True\n",
        "  runs = 5\n",
        "  summary = {}\n",
        "  args.verbose=False\n",
        "  args.plot=False\n",
        "  args.visualize=False\n",
        "\n",
        "  result_summ = {}\n",
        "  for mod in models:\n",
        "    args.model=mod\n",
        "    for loss in losses:\n",
        "      args.loss_type=[loss]\n",
        "      noise_psnr, noise_ssim = 0.0, 0.0 \n",
        "      test_psnr, test_ssim = 0.0, 0.0,\n",
        "      for i in range(runs):\n",
        "        tf.reset_default_graph()\n",
        "        results = train(args)\n",
        "        noise_psnr = noise_psnr + results['noisy_psnr']\n",
        "        noise_ssim = noise_ssim + results['noisy_ssim']\n",
        "        test_psnr = test_psnr + results['test_psnr']\n",
        "        test_ssim = test_ssim + results['test_ssim']\n",
        "      result_summ[mod+'_'+loss] = [test_psnr/runs, test_ssim/runs]\n",
        "      result_summ[mod+'_'+loss+'_noise'] = [noise_psnr/runs, noise_ssim/runs]\n",
        "      print('Done with '+mod+'_'+loss)\n",
        "  return result_summ\n",
        "\n",
        "test_outputs = test_losses()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done with QAE_l2\n",
            "Done with QAE_ssim\n",
            "Done with LAE_l2\n",
            "Done with LAE_ssim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G2IbkohZhqD",
        "colab_type": "code",
        "outputId": "cc68ae49-7aa3-449d-c600-d0beca835a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "test_outputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LAE_l2': [18.676711654663087, 0.5734446048736572],\n",
              " 'LAE_l2_noise': [14.406198883056641, 0.3587961260477702],\n",
              " 'LAE_ssim': [17.16096725463867, 0.6681626319885254],\n",
              " 'LAE_ssim_noise': [14.408073997497558, 0.3585657143592834],\n",
              " 'QAE_l2': [20.20344772338867, 0.6476222395896911],\n",
              " 'QAE_l2_noise': [14.386047554016113, 0.35867895126342775],\n",
              " 'QAE_ssim': [18.381641387939453, 0.7258187294006347],\n",
              " 'QAE_ssim_noise': [14.38495750427246, 0.35857723553975424]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLI5lKwGUP16",
        "colab_type": "code",
        "outputId": "63e207ed-0697-40c5-cf58-bdb5212f0ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def run_simple_models():\n",
        "  models = ['DAE','QAE','LAE','QAE']\n",
        "  runs = 5\n",
        "  summary = {}\n",
        "  args.verbose=False\n",
        "  args.plot=False\n",
        "  args.visualize=False\n",
        "  args.loss_type=['l2']\n",
        "\n",
        "  for j,model in enumerate(models):\n",
        "    args.model=model\n",
        "    if model=='DAE':\n",
        "      args.num_filters = 64\n",
        "      args.epochs = 100\n",
        "    elif j==3:\n",
        "      args.num_filters = 16\n",
        "      args.epochs = 50\n",
        "    else:\n",
        "      args.num_filters = 8\n",
        "      args.epochs = 100\n",
        "    results = 0.0\n",
        "    for i in range(runs):\n",
        "      tf.reset_default_graph()\n",
        "      results = results + np.fromiter(train(args).values(),dtype='float32')\n",
        "    print('Done with model: '+model)\n",
        "    summary[model+str(j)]= results.reshape([2,3])/runs\n",
        "  \n",
        "  return summary\n",
        "\n",
        "simple_test_summary = run_simple_models()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done with model: DAE\n",
            "Done with model: QAE\n",
            "Done with model: LAE\n",
            "Done with model: QAE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3A6Dp35DEgq",
        "colab_type": "code",
        "outputId": "16f6da10-2316-4948-a22a-8a8f025b9030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "simple_test_summary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DAE0': array([[1.4028989e+07, 6.4528197e-01, 1.9823662e+01],\n",
              "        [4.8286948e+07, 3.5890284e-01, 1.4412954e+01]], dtype=float32),\n",
              " 'LAE2': array([[2.4052856e+07, 5.2313924e-01, 1.7467979e+01],\n",
              "        [4.8263640e+07, 3.5902062e-01, 1.4413872e+01]], dtype=float32),\n",
              " 'QAE1': array([[1.5090723e+07, 6.1836278e-01, 1.9567976e+01],\n",
              "        [4.8546536e+07, 3.5876462e-01, 1.4391904e+01]], dtype=float32),\n",
              " 'QAE3': array([[1.4678182e+07, 6.2120342e-01, 1.9637081e+01],\n",
              "        [4.8086248e+07, 3.5889530e-01, 1.4430010e+01]], dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAl0Wa-LCX00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grid_search():\n",
        "  args.plot = False\n",
        "  args.visualize = False\n",
        "  args.epochs = 100\n",
        "  args.verbose = False\n",
        "  args.experiment_name = 'grid_search'\n",
        "\n",
        "  l_rates = [1e-3,6e-4,3e-4,1e-4,6e-5,3e-5,1e-5]\n",
        "  batch_sizes = [10,16,20,32]\n",
        "\n",
        "  avg_loss = {}\n",
        "\n",
        "  for alpha in l_rates:\n",
        "    args.learn_rate = alpha\n",
        "    for b_size in batch_sizes:\n",
        "      args.batch_size = b_size\n",
        "\n",
        "      tf.reset_default_graph()\n",
        "      t_loss, v_loss = train(args)\n",
        "\n",
        "      idx = 'lr_'+str(alpha)+':b_'+str(b_size)\n",
        "\n",
        "      avg_loss[idx] = [t_loss,v_loss]\n",
        "      print('Loss for learn_rate {0:.5f} and batch size {1:2d}, : {2:.6f},{3:.6f}'.format(\n",
        "          alpha,b_size,np.mean(t_loss[-10:]),np.mean(v_loss[-10:])))\n",
        "  return avg_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}